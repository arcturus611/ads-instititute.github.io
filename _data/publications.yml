- year: 2018
  groups:
  - name: Papers
    papers:
    - author: Damek Davis, Dmitriy Drusvyatskiy
      title: Stochastic model-based minimization of weakly convex functions
      journal: arXiv abs/1803.06523
      url: https://arxiv.org/abs/1803.06523
      highlight: This work proves a converge rate of O(k^{-1/4}) for a wide class of algorithms that are based on sampling one sided models of the function. Primary examples are the stochastic subgradient method and the stochastic prox-linear algorithm for minimizing a composition of a convex function with a smooth map.
      
    - author: Damek Davis, Dmitriy Drusvyatskiy, Sham Kakade, Jason D. Lee
      title: Stochastic subgradient method converges on tame functions
      journal: arxiv abs/1804.07795
      url: https://arxiv.org/abs/1804.07795
      highlight: We prove that the stochastic subgradient method, on any ``Whitney stratifiable'' locally Lipschitz function, produces limit points that are all first-order stationary. The class of Whitney stratifiable functions is virtually exhaustive in data science, and in particular, includes all popular deep learning architectures.
   
    - author: Maryam Fazel, Rong Ge, Sham M. Kakade, Mehran Mesbahi
      title: Global Convergence of Policy Gradient Methods for Linearized Control Problems
      journal: arXiv abs/1801.05039
      url: https://arxiv.org/abs/1801.05039
      highlight: This work shows that (model free) policy gradient methods globally converge to the optimal solution and are efficient with regards to their sample and computational complexities.
      
    - author: Damek Davis, Dmitriy Drusvyatskiy
      title: Stochastic subgradient method converges at the rate \(O(k^{−1/4})\) on weakly convex functions
      journal: arXiv abs/1802.02988
      url: https://arxiv.org/abs/1802.02988
      highlight: This work proves that the proximal stochastic subgradient method converges at a rate \(O(k^{-1/4})\) on weakly convex problems. In particular, it resolves the long-standing open question on the rate of convergence of the proximal stochastic gradient method (without batching) for minimizing a sum of a smooth function and a proximable convex function.
      
    - author: Reza Eghbali, James Saunderson, Maryam Fazel
      title: Competitive Online Algorithms for Resource Allocation over the Positive Semidefinite Cone
      journal: arXiv abs/1802.01312
      url: https://arxiv.org/abs/1802.01312
      highlight: Competitive Online Algorithms for Resource Allocation over the Positive Semidefinite Cone
      
    - author: Dmitriy Drusvyatskiy, Maryam Fazel, Scott Roy
      title: An optimal first order method based on optimal quadratic averaging
      journal: arXiv abs/1604.06543
      url: https://arxiv.org/abs/1604.06543
      highlight: An optimal first order method based on optimal quadratic averaging
      
    - author: Amin Jalali, Maryam Fazel, Lin Xiao
      title: Variational Gram Functions- Convex Analysis and Optimization
      journal: arXiv abs/1507.04734
      url: https://arxiv.org/abs/1507.04734
      highlight: This paper considers a new class of convex penalty functions, called variational Gram functions (VGFs), that can promote pairwise relations, such as orthogonality, among a set of vectors in a vector space.

- year: 2017
  groups:
  - name: Survey
    papers:
      - author: Dmitriy Drusvyatskiy, Henry Wolkowicz
        title: The Many Faces of Degeneracy in Conic Optimization
        journal: Foundations and Trends in Optimization, Vol. 3, No. 2, pp 77-170, 2017.
        url: http://www.nowpublishers.com/article/Details/OPT-011
        highlight: This work surveys the roles of Slater's condition and the facial reduction technique in conic optimization.
      
      - author: Dmitriy Drusvyatskiy
        title: The proximal point method revisited
        journal: Survey submitted to SIAG/OPT Views and News
        url: https://arxiv.org/abs/1712.06038
        highlight: This short survey revisits the role of the proximal point method in large scale optimization, focusing on three recent examples&#58; a proximally guided stochastic subgradient method, the prox-linear algorithm, and Catalyst generic acceleration.

  - name: Papers
    papers:
      - author: Hongzhou Lin, Julien Mairal, Zaid Harchaoui
        title: Catalyst Acceleration for First-order Convex Optimization&#58; from Theory to Practice
        journal: arXiv abs/1712.05654
        url: https://arxiv.org/abs/1712.05654
        highlight: The paper presents the comprehensive theory and practice of the Catalyst acceleration scheme for accelerating gradient-based convex optimization methods.<br> Catalyst applies to a large class of optimization algorithms, including gradient descent, block coordinate descent, incremental algorithms such as SAG, SAGA, SDCA, SVRG, Finito/MISO, and their proximal variants.
      
      - author: John Thickstun, Zaid Harchaoui, Dean Foster, Sham M. Kakade
        title: Invariances and Data Augmentation for Supervised Music Transcription
        journal: arXiv abs/1711.04845
        url: https://arxiv.org/abs/1711.04845
        highlight: This paper presents the winning entry to the 2017 MIREX Multiple Fundamental Frequency Estimation evaluation. The method is based on a translation-invariant convolutional architecture.
      
      - author: Damek Davis, Dmitriy Drusvyatskiy, Courtney Paquette
        title: The nonsmooth landscape of phase retrieval
        journal: arXiv abs/1711.03247
        url: https://arxiv.org/abs/1711.03247
        highlight: This paper shows that the standard Polyak subgradient method converges linearly for the phase retrieval problem, when initialized within a constant relative distance of the underlying signal. The second part of the paper analyzes concentration of stationary points of the subsampled objective.
      
      - author: Sébastien Bubeck, Michael B. Cohen, Yin Tat Lee, Yuanzhi Li
        title: An homotopy method for \(\ell_p\) regression provably beyond self-concordance and in input-sparsity time
        journal: arXiv abs/1711.01328
        url: http://arxiv.org/abs/1711.01328
        highlight: Solving two long open problems about $\ell_p$ ball&#58; <br> 1. The existence of input sparsity time algorithm for lp regression. <br> 2. The inexistence of good self-concordance barrier.
        
      - author: Sébastien Bubeck, Michael B. Cohen, James R. Lee, Yin Tat Lee, Aleksander Madry
        title: k-server via multiscale entropic regularization
        journal: arXiv abs/1711.01085
        url: http://arxiv.org/abs/1711.01085
        blog: ./2017/12/20/kserver
        highlight: A breakthough in the k-server problem. The first o(k)-competitive algorithm for hierarchically separated trees.
        
      - author: Yin Tat Lee, Santosh S. Vempala
        title: Convergence Rate of Riemannian Hamiltonian Monte Carlo and Faster Polytope Volume Computation
        journal: arXiv abs/1710.06261
        url: http://arxiv.org/abs/1710.06261
        highlight: A quadratic improvement of polytope volume computation!
        
      - author: Tsz Chiu Kwok, Lap Chi Lau, Yin Tat Lee, Akshay Ramachandran
        title: The Paulsen Problem, Continuous Operator Scaling, and Smoothed Analysis
        journal: arXiv abs/1710.02587
        url: http://arxiv.org/abs/1710.02587
        highlight: Resolving the Paulsen problem - a basic open problem in frame theory with applications from signal processing, communication theory and theoretical computer science.
  
      - author: Hongzhou Lin, Julien Mairal, Zaid Harchaoui
        title: Catalyst Acceleration for Gradient-Based Non-Convex Optimization
        journal: arXiv abs/1703.10993
        url: https://arxiv.org/abs/1703.10993
        highlight: The paper presents the extension of the Catalyst acceleration scheme for gradient <i>non-convex</i> optimization. When the objective is convex, 4WD-Catalyst enjoys the same properties as the regular Catalyst. When the objective is nonconvex, it achieves the best known convergence rate to stationary points for first-order methods.
  
