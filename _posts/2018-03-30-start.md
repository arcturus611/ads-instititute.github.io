---
layout:     post
title:      Stochastic subgradient method converges at the rate \(O(k^{-1/4})\) on weakly convex functions
date:       2018-03-31
summary:    Recent breakthrough on using proximal stochastic gradient method for weakly convex functions.
author:     Damek Davis and Dmitriy Drusvyatskiy
image:      images/stoch_subg_weak_conv.jpeg
image_url:  https://arxiv.org/abs/1802.02988
categories: blog
---

Introduction
============

Stochastic optimization is a fundamental task in the statistical sciences, 
underlying all aspects of learning from data. The goal of stochastic optimization in data science 
is to learn a decision rule from a limited data sample, which generalizes well to the entire population.
Learning such a decision rule amounts to  minimizing the *population risk*: 

$$
\begin{align}\label{eqn:SO}
	\min_{x \in \mathbb{R}^d}~ \mathbb{E}_{\xi\sim P}[f(x,\xi)].\tag{$\mathcal{SO}$}
\end{align}
$$

Here, $$\xi$$ encodes the population data, which is assumed 
to follow some fixed but unknown probability distribution $$P$$, 
and the function $$f(x,\xi)$$ evaluates the loss of the decision rule 
parametrized by $$x$$ on a data point $$\xi$$. 

Robbins-Monro's pioneering 1951 work gave the first procedure 
for solving (\ref{eqn:SO}) when $$f(\cdot, \xi)$$ are smooth and strongly convex, 
inspiring decades of further research. Among such algorithms,
the stochastic (sub)gradient method is the most successful and widely used in practice.
This method constructs a sequence of approximations $$x_t$$ of the minimizer of (\ref{eqn:SO})
by traveling in the direction negative to a sample gradient:

$$
\left\{
	\begin{align*}\label{eqn:SG}
		&\textrm{Sample } \xi_t \sim P \\
		&\textrm{Set } x_{t+1}= x_t - \alpha_t \nabla_x f(x_t, \xi_t)
	\end{align*}
\right\},\tag{$\mathcal{SG}$}
$$

where $$\alpha_t>0$$ is an appropriately chosen control sequence. 
Nonsmooth convex problems may be similarly optimized by replacing sample gradients 
by sample subgradients $$ v_t\in \partial_x f(x_t,\xi_t)$$, where
$$\partial_x f(x_t, \xi_t)$$ is the subdifferential
in the sense of convex analysis; see for example, R.T. Rockafellar (1970, Part V.)


Performance of stochastic optimization methods is best judged 
by their *sample complexity* -- the number of i.i.d. realizations
$$\xi_1, \ldots, \xi_N \sim P$$ needed to reach a desired accuracy of the decision rule.
Classical results such as by Nemirovsky and Yudin (1983) 
stipulate that for convex problems, it suffices to generate
$$O(\varepsilon^{-2})$$ samples to reach functional accuracy $$\varepsilon$$ in expectation, 
and this complexity is unimprovable without making stronger assumptions. 


While the sample complexity of the stochastic subgradient method is well-understood for convex problems,
much less is known for nonconvex problems. In particular, 
the sample complexity of the method is not yet known for any 
reasonably wide class of problems beyond those that are smooth or convex.
This is somewhat concerning as the stochastic subgradient method is 
the simplest and most widely used stochastic optimization algorithm for 
large-scale problems arising in machine learning and is the core optimization subroutine 
in industry backed software libraries, such as Google's TensorFlow.

In the recent paper (Davis and Drusvyatskiy, 2018), we aim to close the gap 
between theory and practice and provide the first sample complexity bounds 
for the stochastic subgradient method applied to a large class 
of nonsmooth and nonconvex  optimization problems. The problem class we consider 
captures a variety of important computational tasks in data science, as described below.


Our guarantees apply to an even broader setting than population risk minimization (\ref{eqn:SO}). 
Indeed, numerous tasks in machine learning and high dimensional statistics yield problems of the form

$$
\begin{equation}\label{eqn:gen_err}
	\min_{x\in\mathbb{R}^d}~ \varphi(x)=g(x)+r(x),\tag{$\mathcal{P}$}
\end{equation}
$$

where the functional components $$g$$ and $$r$$ play qualitatively different roles. 
The function $$g\colon\mathbb{R}^d\to\mathbb{R}$$ plays a similar role 
to the population risk in (\ref{eqn:SO}). We will assume 
that the only access to $$g$$ is through stochastic estimates of 
its (generalized) gradients. That is,  given a point $$x$$, one can
generate a random vector $$v\in\mathbb{R}^d$$ satisfying 
$$\mathbb{E}[v]\in \partial g(x)$$. A formal definition of the 
nonconvex subdifferential $$\partial g(x)$$ is standard in
the optimization literature; see Definition 8.3 in (Rockafellar and Wets, 1998). 
The exact details will not be important for the blog. 
We note however that when $$g$$ is differentiable at $$x$$, 
the subdifferential $$\partial g(x)$$ consists only of the gradient 
$$\nabla g(x)$$, while for convex functions, 
it reduces to the subdifferential in the sense of convex analysis.


In contrast, we assume the function $$r\colon\mathbb{R}^d\to\mathbb{R}\cup\{+\infty\}$$
to be explicitly known and simple. It is often used to model constraints on the parameters
 $$x$$ or to encourage $$x$$ to have some low dimensional structure, such as sparsity or low rank.
 Within a Bayesian framework, the regularizer $$r$$ can model prior distributional information on 
 $$x$$. One common assumption, which we also make here, is that
 $$r$$ is closed and convex and admits a computable proximal map 
 
 $$
 \begin{equation*}
	{\rm prox}_{\alpha r}(x):= \underset{y}{\operatorname{argmin}}\, \left\{r(y)+\tfrac{1}{2\alpha}\|y-x\|^2\right\}.
\end{equation*}
$$

In particular, when $$r$$ is an indicator function of a closed convex set 
-- meaning it equals zero on it and is $$+\infty$$ off it -- 
the proximal map $${\rm prox}_{\alpha r}(\cdot)$$ reduces to the nearest-point projection.

The most widely used algorithm  for (\ref{eqn:gen_err}) is a direct generalization of (\ref{eqn:SG}), 
called the *proximal stochastic subgradient method*. Given a current iterate $$x_t$$, the
method performs the update

$$
\begin{equation*}\left\{
	\begin{aligned}
		&\textrm{Generate a stochastic subgradient } v_t\in\mathbb{R}^d \textrm{ of } g \textrm{ at } x_t\\
		& \textrm{Set } x_{t+1}={\rm prox}_{\alpha_t r}\left(x_{t} - \alpha_t v_t\right)
	\end{aligned}\right\},
\end{equation*}
$$ 

where $$\alpha_t>0$$ is an appropriately chosen control sequence.

The search for stationary points
============

Convex optimization algorithms are judged by the rate at which they decrease the function value along the iterate sequence. Analysis of smooth optimization algorithms  focuses instead on the magnitude of the gradients along the iterates. The situation becomes quite different for problems that are neither smooth nor convex.

The primary goal, akin to smooth minimization, is the search for stationary points. 
A point $$x\in\mathbb{R}^d$$ is called *stationary* for the problem (\ref{eqn:gen_err}) if the inclusion $$0\in \partial \varphi(x)$$ holds. In "primal terms", these are precisely the points where the directional derivative of $$\varphi$$ is nonnegative in every direction. Indeed, under mild conditions on $$\varphi$$, equality holds; see Proposition 8.32 in (Rockafellar and Wets, 1998):

$$
\begin{equation*} %\label{eqn:subdif_direc_der}
	{\rm dist}(0;\partial \varphi(x))=-\inf_{v:\, \|v\|\leq 1} \varphi'(x;v).
\end{equation*}
$$

Thus a point $$x$$, satisfying $${\rm dist}(0;\partial \varphi(x))\leq \varepsilon$$, approximately satisfies first-order necessary conditions for optimality.


An immediate difficulty in analyzing stochastic subgradient methods for nonsmooth and nonconvex problems is that it is not a priori clear how to measure the progress of the algorithm. Neither the functional suboptimality gap, $$\varphi(x_t)-\min \varphi$$, nor the stationarity measure, $${\rm dist}(0;\partial \varphi(x_t))$$, necessarily tend to zero along the iterate sequence. Indeed, what is missing is a continuous measure of stationarity to monitor, instead of the highly discontinuous function $$x\mapsto{\rm dist}(0;\partial \varphi(x))$$.


References
==========
R.T. Rockafellar and R.J-B. Wets. *Variational Analysis*. Grundlehren der mathemtischen Wissenschaften, Vol 317, Springer, Berlin, 1998.

